{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14d29ebd3592ecb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Late Interaction Text Embedding Models\n",
    "\n",
    "As of version 0.3.0 FastEmbed supports Late Interaction Text Embedding Models and currently available with one of the most popular embedding model of the family - ColBERT.\n",
    "\n",
    "## What is a Late Interaction Text Embedding Model?\n",
    "\n",
    "Late Interaction Text Embedding Model is a kind of information retrieval model which performs query and documents interactions at the scoring stage.\n",
    "In order to better understand it, we can compare it to the models without interaction.  \n",
    "For instance, if you take a sentence-transformer model, compute embeddings for your documents, compute embeddings for your queries, and just compare them by cosine similarity, then you're retrieving points without interaction.\n",
    "\n",
    "It is a pretty much easy and straightforward approach, however we might be sacrificing some precision due to its simplicity. It is caused by several facts: \n",
    "- there is no interaction between queries and documents at the early stage (embedding generation) nor at the late stage (during scoring). \n",
    "- we are trying to encapsulate all the document information in only one pooled embedding, and obviously, some information might be lost.\n",
    "\n",
    "Late Interaction Text Embedding models are trying to address it by computing embeddings for each token in queries and documents, and then finding the most similar ones via model specific operation, e.g. ColBERT (Contextual Late Interaction over BERT) uses MaxSim operation.\n",
    "With this approach we can have not only a better representation of the documents, but also make queries and documents more aware one of another.\n",
    "\n",
    "For more information on ColBERT and MaxSim operation, you can check out [this blogpost](https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/) by Jina AI.\n",
    "\n",
    "## ColBERT in FastEmbed\n",
    "\n",
    "FastEmbed provides a simple way to use ColBERT model, similar to the ones it has with `TextEmbedding`.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1053b17c810be5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:20:26.927643Z",
     "start_time": "2024-06-03T17:20:25.128994Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/envs/fastembed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model': 'yazge/turkish-colbert-onnx',\n",
       "  'dim': 768,\n",
       "  'description': 'Turkish ColBERT ONNX model',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.5,\n",
       "  'sources': {'hf': 'yazge/turkish-colbert-onnx'},\n",
       "  'model_file': 'onnx/model.onnx'},\n",
       " {'model': 'colbert-ir/colbertv2.0',\n",
       "  'dim': 128,\n",
       "  'description': 'Late interaction model',\n",
       "  'license': 'mit',\n",
       "  'size_in_GB': 0.44,\n",
       "  'sources': {'hf': 'colbert-ir/colbertv2.0'},\n",
       "  'model_file': 'model.onnx'},\n",
       " {'model': 'answerdotai/answerai-colbert-small-v1',\n",
       "  'dim': 96,\n",
       "  'description': 'Text embeddings, Unimodal (text), Multilingual (~100 languages), 512 input tokens truncation, 2024 year',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.13,\n",
       "  'sources': {'hf': 'answerdotai/answerai-colbert-small-v1'},\n",
       "  'model_file': 'vespa_colbert.onnx'},\n",
       " {'model': 'jinaai/jina-colbert-v2',\n",
       "  'dim': 128,\n",
       "  'description': 'New model that expands capabilities of colbert-v1 with multilingual and context length of 8192, 2024 year',\n",
       "  'license': 'cc-by-nc-4.0',\n",
       "  'size_in_GB': 2.24,\n",
       "  'sources': {'hf': 'jinaai/jina-colbert-v2'},\n",
       "  'model_file': 'onnx/model.onnx',\n",
       "  'additional_files': ['onnx/model.onnx_data']}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastembed import LateInteractionTextEmbedding\n",
    "\n",
    "LateInteractionTextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c15893df422631",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:23:35.764183Z",
     "start_time": "2024-06-03T17:23:21.630277Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_model = LateInteractionTextEmbedding(\"yazge/turkish-colbert-onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e560b5fa7d63bea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:39:33.400876Z",
     "start_time": "2024-06-03T17:39:33.397431Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"ColBERT is a late interaction text embedding model, however, there are also other models such as TwinBERT.\",\n",
    "    \"On the contrary to the late interaction models, the early interaction models contains interaction steps at embedding generation process\",\n",
    "]\n",
    "queries = [\n",
    "    \"Are there any other late interaction text embedding models except ColBERT?\",\n",
    "    \"What is the difference between late interaction and early interaction text embedding models?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ad924a3449743",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*NOTE*: ColBERT computes query and documents embeddings differently, make sure to use the corresponding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "496fbf51e4eaaae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:39:34.379885Z",
     "start_time": "2024-06-03T17:39:34.316257Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2025-02-12 12:22:35.941086094 [E:onnxruntime:, sequential_executor.cc:516 ExecuteKernel] Non-zero status code returned while running Add node. Name:'/embeddings/Add' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/math/element_wise_ops.h:560 void onnxruntime::BroadcastIterator::Append(ptrdiff_t, ptrdiff_t) axis == 1 || axis == largest was false. Attempting to broadcast an axis by a dimension other than 1. 41 by 42\n",
      "\u001b[m\n"
     ]
    },
    {
     "ename": "RuntimeException",
     "evalue": "[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Add node. Name:'/embeddings/Add' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/math/element_wise_ops.h:560 void onnxruntime::BroadcastIterator::Append(ptrdiff_t, ptrdiff_t) axis == 1 || axis == largest was false. Attempting to broadcast an axis by a dimension other than 1. 41 by 42\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m document_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m  \u001b[38;5;66;03m# embed and qury_embed return generators,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# which we need to evaluate by writing them to a list\u001b[39;00m\n\u001b[1;32m      5\u001b[0m query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(embedding_model\u001b[38;5;241m.\u001b[39mquery_embed(queries))\n",
      "File \u001b[0;32m~/miniconda3/envs/fastembed/lib/python3.10/site-packages/fastembed/late_interaction/late_interaction_text_embedding.py:99\u001b[0m, in \u001b[0;36mLateInteractionTextEmbedding.embed\u001b[0;34m(self, documents, batch_size, parallel, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     documents: Union[\u001b[38;5;28mstr\u001b[39m, Iterable[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     83\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[NumpyArray]:\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    Encode a list of documents into list of embeddings.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    We use mean pooling with attention so that the model can handle variable-length inputs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m        List of embeddings, one per document\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed(documents, batch_size, parallel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fastembed/lib/python3.10/site-packages/fastembed/late_interaction/colbert.py:250\u001b[0m, in \u001b[0;36mColbert.embed\u001b[0;34m(self, documents, batch_size, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed\u001b[39m(\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    230\u001b[0m     documents: Union[\u001b[38;5;28mstr\u001b[39m, Iterable[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    234\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[NumpyArray]:\n\u001b[1;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    Encode a list of documents into list of embeddings.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    We use mean pooling with attention so that the model can handle variable-length inputs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m        List of embeddings, one per document\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_documents(\n\u001b[1;32m    251\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m    252\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dir),\n\u001b[1;32m    253\u001b[0m         documents\u001b[38;5;241m=\u001b[39mdocuments,\n\u001b[1;32m    254\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    255\u001b[0m         parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[1;32m    256\u001b[0m         providers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproviders,\n\u001b[1;32m    257\u001b[0m         cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda,\n\u001b[1;32m    258\u001b[0m         device_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    260\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/fastembed/lib/python3.10/site-packages/fastembed/text/onnx_text_model.py:119\u001b[0m, in \u001b[0;36mOnnxTextModel._embed_documents\u001b[0;34m(self, model_name, cache_dir, documents, batch_size, parallel, providers, cuda, device_ids, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_onnx_model()\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(documents, batch_size):\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_onnx_output(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/fastembed/lib/python3.10/site-packages/fastembed/text/onnx_text_model.py:86\u001b[0m, in \u001b[0;36mOnnxTextModel.onnx_embed\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     onnx_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m     82\u001b[0m         [np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(e), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m input_ids], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     84\u001b[0m onnx_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_onnx_input(onnx_input, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 86\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mONNX_OUTPUT_NAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OnnxOutputContext(\n\u001b[1;32m     88\u001b[0m     model_output\u001b[38;5;241m=\u001b[39mmodel_output[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     89\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39monnx_input\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, attention_mask),\n\u001b[1;32m     90\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39monnx_input\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_ids),\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/fastembed/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:266\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    264\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mRuntimeException\u001b[0m: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Add node. Name:'/embeddings/Add' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/math/element_wise_ops.h:560 void onnxruntime::BroadcastIterator::Append(ptrdiff_t, ptrdiff_t) axis == 1 || axis == largest was false. Attempting to broadcast an axis by a dimension other than 1. 41 by 42\n"
     ]
    }
   ],
   "source": [
    "document_embeddings = list(\n",
    "    embedding_model.embed(documents)\n",
    ")  # embed and qury_embed return generators,\n",
    "# which we need to evaluate by writing them to a list\n",
    "query_embeddings = list(embedding_model.query_embed(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50595bb0498f0c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:39:34.793528Z",
     "start_time": "2024-06-03T17:39:34.788545Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26, 128), (32, 128))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_embeddings[0].shape, query_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e43f2c24a7d5fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Don't worry about query embeddings having the bigger shape in this case. \n",
    "ColBERT authors recommend to pad queries with [MASK] tokens to 32 tokens.\n",
    "They also recommends to truncate queries to 32 tokens, however we don't do that in FastEmbed, so you can put some straight into the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a4011effd3699",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## MaxSim operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea4cf82521f2de",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Qdrant will support ColBERT as of the next version (v1.10), however, at the moment, you can compute embedding similarities manually.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f84392f63d2c6076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:39:36.431622Z",
     "start_time": "2024-06-03T17:39:36.427363Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_relevance_scores(\n",
    "    query_embedding: np.array, document_embeddings: np.array, k: int\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Compute relevance scores for top-k documents given a query.\n",
    "\n",
    "    :param query_embedding: Numpy array representing the query embedding, shape: [num_query_terms, embedding_dim]\n",
    "    :param document_embeddings: Numpy array representing embeddings for documents, shape: [num_documents, max_doc_length, embedding_dim]\n",
    "    :param k: Number of top documents to return\n",
    "    :return: Indices of the top-k documents based on their relevance scores\n",
    "    \"\"\"\n",
    "    # Compute batch dot-product of query_embedding and document_embeddings\n",
    "    # Resulting shape: [num_documents, num_query_terms, max_doc_length]\n",
    "    scores = np.matmul(query_embedding, document_embeddings.transpose(0, 2, 1))\n",
    "\n",
    "    # Apply max-pooling across document terms (axis=2) to find the max similarity per query term\n",
    "    # Shape after max-pool: [num_documents, num_query_terms]\n",
    "    max_scores_per_query_term = np.max(scores, axis=2)\n",
    "\n",
    "    # Sum the scores across query terms to get the total score for each document\n",
    "    # Shape after sum: [num_documents]\n",
    "    total_scores = np.sum(max_scores_per_query_term, axis=1)\n",
    "\n",
    "    # Sort the documents based on their total scores and get the indices of the top-k documents\n",
    "    sorted_indices = np.argsort(total_scores)[::-1][:k]\n",
    "\n",
    "    return sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c61d07bed7b60e35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:39:37.053383Z",
     "start_time": "2024-06-03T17:39:37.050926Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted document indices: [0 1]\n"
     ]
    }
   ],
   "source": [
    "sorted_indices = compute_relevance_scores(\n",
    "    np.array(query_embeddings[0]), np.array(document_embeddings), k=3\n",
    ")\n",
    "print(\"Sorted document indices:\", sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b24df2569970d9e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T17:40:52.276846Z",
     "start_time": "2024-06-03T17:40:52.273789Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Are there any other late interaction text embedding models except ColBERT?\n",
      "Document: ColBERT is a late interaction text embedding model, however, there are also other models such as TwinBERT.\n",
      "Document: On the contrary to the late interaction models, the early interaction models contains interaction steps at embedding generation process\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {queries[0]}\")\n",
    "for index in sorted_indices:\n",
    "    print(f\"Document: {documents[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de537c37aff3927",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Use-case recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3525d3259cd2b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Despite ColBERT allows to compute embeddings independently and spare some workload offline, it still computes more resources than no interaction models. Due to this, it might be more reasonable to use ColBERT not as a first-stage retriever, but as a re-ranker.\n",
    "\n",
    "The first-stage retriever would then be a no-interaction model, which e.g. retrieves first 100 or 500 examples, and leave the final ranking to the ColBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa922793454b4ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastembed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
